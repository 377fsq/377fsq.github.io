

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="https://fsq.oss-cn-hangzhou.aliyuncs.com/favicon.png">
  <link rel="icon" href="https://fsq.oss-cn-hangzhou.aliyuncs.com/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="ViTDet 视觉领域Transformer骨干  一句话总结将普通 ViT 主干预训练为MAE，检测器 ViTDet 可以与之前所有基于分层主干的领先方法竞争，仅使用 ImageNet-1K预训练在 COCO上可达到 61.3 box AP！代码将开源！  MAE是使用类似于BERT的掩码机制，从图片中随机抹去一些像素，并让模型通过已知像素去构建未知像素，从而迫使模型学习图像中的特征。  一、摘">
<meta property="og:type" content="article">
<meta property="og:title" content="ViTDet">
<meta property="og:url" content="http://example.com/2022/04/12/ViTDet/index.html">
<meta property="og:site_name" content="七">
<meta property="og:description" content="ViTDet 视觉领域Transformer骨干  一句话总结将普通 ViT 主干预训练为MAE，检测器 ViTDet 可以与之前所有基于分层主干的领先方法竞争，仅使用 ImageNet-1K预训练在 COCO上可达到 61.3 box AP！代码将开源！  MAE是使用类似于BERT的掩码机制，从图片中随机抹去一些像素，并让模型通过已知像素去构建未知像素，从而迫使模型学习图像中的特征。  一、摘">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fsq.oss-cn-hangzhou.aliyuncs.com/image-20220412143118054.png">
<meta property="article:published_time" content="2022-04-12T06:29:06.000Z">
<meta property="article:modified_time" content="2022-04-12T06:49:15.550Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://fsq.oss-cn-hangzhou.aliyuncs.com/image-20220412143118054.png">
  
  
  <title>ViTDet - 七</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":true},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 6.1.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>琦琦怪怪🎈</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="ViTDet">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-04-12 14:29" pubdate>
        2022年4月12日 下午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      3.7k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      31 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">ViTDet</h1>
            
            <div class="markdown-body">
              <h1 id="ViTDet"><a href="#ViTDet" class="headerlink" title="ViTDet"></a>ViTDet</h1><blockquote>
<p>视觉领域Transformer骨干</p>
</blockquote>
<h2 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h2><p>将普通 ViT 主干预训练为<strong>MAE</strong>，检测器 ViTDet 可以与之前所有基于分层主干的领先方法竞争，仅使用 ImageNet-1K预训练在 COCO上可达到 61.3 box AP！代码将开源！</p>
<ul>
<li>MAE是使用类似于BERT的掩码机制，从图片中随机抹去一些像素，并让模型通过已知像素去构建未知像素，从而迫使模型学习图像中的特征。</li>
</ul>
<h2 id="一、摘要"><a href="#一、摘要" class="headerlink" title="一、摘要"></a>一、摘要</h2><p>“We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection.” (Li 等。, 2022, p. 1)</p>
<ul>
<li>以最普通的，非层级结构的ViT作为Baseline改进。</li>
<li>基于ViT设计一个针对目标检测任务的通用骨干。</li>
</ul>
<p>本文主要目标是消除骨干网络中的分层结构约束，主要探索一种适用于目标检测的普通骨干网络。</p>
<h2 id="二、引言"><a href="#二、引言" class="headerlink" title="二、引言"></a>二、引言</h2><ul>
<li><p>第一段，拆分检测网络结构backbone+neck+head</p>
<p>  引出问题：</p>
<p>  “For a long while, these backbones have been multi-scale, hierarchical architectures due to the de facto design of convolutional networks (ConvNet) [31], which has heavily influenced the neck&#x2F;head design for detecting objects at multiple scales (e.g., FPN).” (Li 等。, 2022, p. 1)</p>
<p>  卷积网络的backbone大多都是<strong>多尺度</strong>设计，这严重影响了neck&#x2F;head的设计</p>
</li>
<li><p>第二段，引出“Vision Transformers (ViT)”</p>
<p>  ViT相对卷积网络的不同（主要是针对第一段问题展开）：原始的ViT是一种普通的、非分层的结构。满足极简架构的需求。</p>
<p>  引出问题：</p>
<p>  “How can we address multi-scale objects in a downstream task with a plain backbone from upstream pre-training? Is a plain ViT too inefficient to use with high-resolution detection images?” (Li 等。, 2022, p. 1)</p>
<p>  如何应对下游任务的多尺寸对象？低效普通的ViT能够检测高分辨率图像？</p>
<p>  解决办法：将层级设计引入回backbone。</p>
<p>  相关研究：</p>
<p>  “Swin Transformers [40] and related works [53,16,32,28]”</p>
</li>
<li><p>第三-五段，引出自己的研究内容。</p>
<ol>
<li><p>提出设计:从独立上下游任务考虑，Figure 1右图为作者团队的设计：在最后一个特征图构建一个简单的金字塔neck。</p>
<p> 惊讶结论①：<strong>从单尺度特征图（没有常见的 FPN 设计）构建一个简单的特征金字塔就足够了；</strong></p>
<p> 惊讶结论②：<strong>使用窗口注意力（没有移位）就足够了跨窗口传播块</strong></p>
</li>
<li><p>效果优越，带有MAE效果更好。</p>
</li>
<li><p>展望</p>
</li>
</ol>
</li>
</ul>
<h2 id="三、相关工作"><a href="#三、相关工作" class="headerlink" title="三、相关工作"></a>三、相关工作</h2><ul>
<li>目标检测骨干网络</li>
<li>普通骨干网络的检测器</li>
<li>目标检测方法</li>
</ul>
<h2 id="四、方法介绍"><a href="#四、方法介绍" class="headerlink" title="四、方法介绍"></a>四、方法介绍</h2><p><em>研究目标</em>：删除backbone上的分层约束。做最小的微调适用于目标检测任务。</p>
<ul>
<li><p>简单的特征金字塔</p>
<ol>
<li><p><strong>仅从backbone最后一层的特征图，构建一个简单的特征金字塔</strong></p>
</li>
<li><p>“However, our scenario involves upsampling from a deep, lowresolution feature map, unlike [38], which taps into shallower feature maps. In hierarchical backbones, upsampling is often aided by lateral connection [35]; in plain ViT backbones, we empirically find this is not necessary (Sec. 4) and simple deconvolutions are sufficient.”</p>
<p> <strong>不需要自上而下的连接</strong></p>
</li>
<li><p>比较了两种种FPN变体(a)(b)</p>
<p> <img src="https://fsq.oss-cn-hangzhou.aliyuncs.com/image-20220412143118054.png" srcset="/img/loading.gif" lazyload alt="image-20220412143118054"></p>
<ul>
<li><ol>
<li>具体实现-&gt;在最后一个特征图上并行应用一组卷积或反卷积来生成多尺度特征图。具体来说，使用标准的ViT步长stride&#x3D;16，然后利用下采样卷积得到1&#x2F;32,1&#x2F;16的尺度特征图，利用反卷积操作得到1&#x2F;8,1&#x2F;4尺度特征图。</li>
</ol>
</li>
</ul>
</li>
</ol>
</li>
<li><p>骨干网络适应</p>
<p>  针对问题</p>
<p>  1、目标检测受益于高分辨率输入图像，但是这个又与在Transformer中全局计算自注意力存在矛盾，因为这样会变得更慢，并且占据很多内存。</p>
<p>  解决方法</p>
<p>  “We explore using window attention [52] with a few cross-window blocks. During fine-tuning, given a high-resolution feature map, we divide it into regular non-overlapping windows.5 Self-attention is computed within each window. This is referred to as “restricted ” self-attention in the original Transformer [52].” (Li 等。, 2022, p. 6)</p>
<p>  本文还采用几个跨窗口的模块来计算Window attention,对于给定高分辨率特征图我们将其划分为常规的非重叠窗口，在每个窗口内计算自注意力。</p>
<p>  与 Swin 不同，我们不会跨层“移动”窗口。为了允许信息传播，我们使用了极少数（默认情况下，4 个）可以传播的块。我们将预训练的主干网络平均分成 4 个块子集（例如，对于 24 块 ViT-L，每个子集中有 6 个）。我们在每个子集的最后一个块中应用传播策略。</p>
<ol>
<li><p>策略一：全局传播</p>
<p> 在每个子集的最后一个块中执行全局自我注意。由于全局块的数量很少，内存和计算成本是可行的。</p>
</li>
<li><p>策略二：卷积传播</p>
<p> 作为替代方案，我们在每个子集之后添加一个额外的卷积块。卷积块是一个残差块，由一个或多个卷积和一个恒等连接操作组成。该块中的最后一层被初始化为零，因此该块的初始状态是一个标记。将块初始化为身份允许我们将其插入到预训练主干中的任何位置，而不会破坏主干的初始状态。</p>
</li>
</ol>
</li>
<li><p>实现</p>
<ol>
<li>patch size &#x3D; 16*16</li>
<li>输入尺寸 1024*1024</li>
<li>AdamW优化器</li>
</ol>
</li>
</ul>
<h2 id="五、实验"><a href="#五、实验" class="headerlink" title="五、实验"></a>五、实验</h2><p>默认backbone情况是：vit+简单金字塔+全局传播（4个传播块）</p>
<p>预训练：backbone+MAE无标签在IN-1k上进行预训练</p>
<h3 id="实验一：证明简单金字塔是有效的"><a href="#实验一：证明简单金字塔是有效的" class="headerlink" title="实验一：证明简单金字塔是有效的"></a>实验一：证明简单金字塔是有效的</h3><p>消融对比试验，baseline是没有金字塔，abc分别对应上面三种图的情况。实验结果表明作者提出的简单金字塔效果最好。</p>
<h3 id="实验二：证明窗口注意仅需很少的传播块"><a href="#实验二：证明窗口注意仅需很少的传播块" class="headerlink" title="实验二：证明窗口注意仅需很少的传播块"></a>实验二：证明窗口注意仅需很少的传播块</h3><p>backbone适应消融实验</p>
<p>(a)不同的跨窗口交互策略：无、4个全局传播块、<strong>4个卷积传播块</strong>、移位窗口</p>
<p>(b)探究了不同类型的卷积块应用于卷积传播【<strong>basic two 3x*3;*</strong> bottleneck 1x1-&gt;3x3-&gt;1x1; naive 一个1x1卷积 】</p>
<p>(c)探究跨窗口传播应该位于骨干网络的哪个位置。【放在最前面4个块；放在最后4个块；<strong>4个全局传播块均匀的放位置。</strong>】</p>
<p>(d)探究使用的全局传播的块数</p>
<h3 id="实验三：“Comparisons-of-plain-vs-hierarchical-backbones”"><a href="#实验三：“Comparisons-of-plain-vs-hierarchical-backbones”" class="headerlink" title="实验三：“Comparisons of plain vs. hierarchical backbones”"></a>实验三：“Comparisons of plain vs. hierarchical backbones”</h3><p>金字塔Backbone对比</p>
<h3 id="实验四：与其他分层结构的backbone比较"><a href="#实验四：与其他分层结构的backbone比较" class="headerlink" title="实验四：与其他分层结构的backbone比较"></a>实验四：与其他分层结构的backbone比较</h3><h2 id="总结-amp-感悟"><a href="#总结-amp-感悟" class="headerlink" title="总结&amp;感悟"></a>总结&amp;感悟</h2><p>videt还是想解耦预训练和检测</p>
<ol>
<li><p>继承了来自<strong>YOLOF</strong>去掉FPN的思考，这次的<strong>ViTDet</strong>选择了直接暴力抹掉<strong>shift window</strong>，一步到位。</p>
</li>
<li><p>训练空间还是挺夸张的，不适合小老百姓的使用。采用MAE来无监督预训练，所以论文看起来简单，但是也只有大厂能烧的起实验。感觉这更多是一种工程化的探索，以及对于MAE的一次推广吧，所以读一读就可，深入研究的话，可能不一定能够跑得起来。</p>
</li>
<li><p>关于论文，有点不清不楚，怎么分成子集？平行训练？如果要更改结构预训练权重能否迁移？会不会效果不好？</p>
</li>
<li><p>只能说东一个大致更改的位置，但是对于细节不明不白。。。</p>
</li>
<li><p>尚未开源</p>
</li>
<li><p>yolof论文证明了FPN多尺度是没有用的。居然没引用。</p>
</li>
<li><p>一个证明，拿Swin来做backbone的话，window size的self-attention就够了，没有必要用shift，用一个普通的带shortcut的卷积来代替shift操作就行了。就是attention+convolution效果极好。</p>
</li>
<li><p>一个有意思的点：该论文中<code>Window attention + global propagation block</code>的设定和 <code>Transformer in Transformer</code> 的思路是一致的。</p>
<blockquote>
<p><strong>Window attention &#x3D;&#x3D; inner transformer</strong><br><strong>global propagation block &#x3D;&#x3D; outer transformer</strong><br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.00112">https://arxiv.org/abs/2103.00112</a></p>
</blockquote>
</li>
</ol>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E8%AE%BA%E6%96%87/">论文</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Transformer/">Transformer</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/04/11/MAE/">
                        <span class="hidden-mobile">MAE</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"ezAjSCt600hHMv0BReBYtgEg-gzGzoHsz","appKey":"ED6lqUmM45dvAhwFci9k1ioL","path":"window.location.pathname","placeholder":"欢迎留言呀~","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          Fluid.plugins.initFancyBox('#valine .vcontent img:not(.vemoji)');
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>












  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
