

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="PVTv1  动机 ViT 的提出是创造性的，但是同时也存在许多可以改进的空间。比如说很难直接将 ViT 应用在一些下游任务，原因是什么？一方面是其存在网络设计的问题，还有一方面原因是其很高的显存占用。 这个工作把金字塔结构引入到Transformer中，使得它可以像ResNet那样无缝接入到各种下游任务中（如：物体检测，语义分割）  一、分析ViT遗留的问题 网络设计的问题">
<meta property="og:type" content="article">
<meta property="og:title" content="PVT">
<meta property="og:url" content="http://example.com/2022/05/09/PVT/index.html">
<meta property="og:site_name" content="方小七">
<meta property="og:description" content="PVTv1  动机 ViT 的提出是创造性的，但是同时也存在许多可以改进的空间。比如说很难直接将 ViT 应用在一些下游任务，原因是什么？一方面是其存在网络设计的问题，还有一方面原因是其很高的显存占用。 这个工作把金字塔结构引入到Transformer中，使得它可以像ResNet那样无缝接入到各种下游任务中（如：物体检测，语义分割）  一、分析ViT遗留的问题 网络设计的问题">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fsq.oss-cn-hangzhou.aliyuncs.com/%E7%89%B9%E5%BE%81%E5%9B%BE%E5%88%86%E8%BE%A8%E7%8E%87%E7%9A%84%E5%BD%B1%E5%93%8D.jpg">
<meta property="article:published_time" content="2022-05-09T07:22:03.000Z">
<meta property="article:modified_time" content="2022-05-11T11:20:02.754Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://fsq.oss-cn-hangzhou.aliyuncs.com/%E7%89%B9%E5%BE%81%E5%9B%BE%E5%88%86%E8%BE%A8%E7%8E%87%E7%9A%84%E5%BD%B1%E5%93%8D.jpg">
  
  
  <title>PVT - 方小七</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":true},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 6.1.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>琦琦怪怪🎈</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="PVT">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-05-09 15:22" pubdate>
        2022年5月9日 下午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      4.2k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      35 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">PVT</h1>
            
            <div class="markdown-body">
              <h1 id="pvtv1">PVTv1</h1>
<blockquote>
<p><strong>动机</strong></p>
<p>ViT
的提出是创造性的，但是同时也存在许多可以改进的空间。比如说<strong>很难直接将
ViT
应用在一些下游任务</strong>，原因是什么？一方面是其存在网络设计的问题，还有一方面原因是其很高的显存占用。</p>
<p>这个工作把金字塔结构引入到Transformer中，使得它可以像ResNet那样无缝接入到各种下游任务中（如：物体检测，语义分割）</p>
</blockquote>
<h3 id="一分析vit遗留的问题"><strong>一、分析ViT遗留的问题</strong></h3>
<h3 id="网络设计的问题"><strong>网络设计的问题</strong></h3>
<p>我们知道 ViT 在输入端做了切图，得到一个 16-stride 或者 32-stride
的特征图，网络中特征图大小就不变了，<strong>整体上是一个柱状结构</strong>。问题是
16-stride 或者 32-stride
的选择对于不同任务是否都是合理的？答案显然是否定的。</p>
<ul>
<li>ViT
特征图的分辨率取决于输入端切图大小的设置，然而不同图片或者说不同任务对分辨率的需求是不一样的。</li>
<li>越复杂的图片或越复杂的任务，对分辨率的需求就越高，苹果有 2x2
的分辨率就够了，海鸥则需要 3x3 的分辨率，宫殿就需要 4x4 的分辨率。</li>
</ul>
<p><img src="https://fsq.oss-cn-hangzhou.aliyuncs.com/%E7%89%B9%E5%BE%81%E5%9B%BE%E5%88%86%E8%BE%A8%E7%8E%87%E7%9A%84%E5%BD%B1%E5%93%8D.jpg" srcset="/img/loading.gif" lazyload alt="特征图分辨率的影响" style="zoom:50%;" /></p>
<h3 id="显存占用问题"><strong>显存占用问题</strong></h3>
<p>ViT
还有一个问题就是显存占用高，碍于显存的问题很难处理高分辨率的输入图片。对于分类任务输入图片
224x224
的分辨率可能就够了，但是对于分割任务、目标检测这类任务我们往往需要更高分辨率的输入图片。</p>
<p>ViT 的显存占用率高很大程度上是由于 Attention
计算的需求。参考这篇文章<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/89206353">GPU和显存分析</a>，我们估算一下
Attention 计算的显存需求。</p>
<p>如果 input_size=224x224, num_patches=4x4, attention 计算公式为</p>
<p>$ Attention(Q,K,V) = Softmax()V$</p>
<ul>
<li>那么 QKV 各自会有<span
class="math inline">\((\frac{224}{4})^2=56\times56=3136\)</span>个
token</li>
<li>计算<span class="math inline">\(Q\times K\)</span>占用的显存为<span
class="math inline">\(3136\times 3136\times num_{heads}\times 4B\approx
37.5M\times num_{heads}\)</span></li>
</ul>
<p>如果将 input_size 上升到 800x800，同理计算<span
class="math inline">\(Q\times K\)</span>占用的显存为<span
class="math inline">\(40000\times 40000\times num_{heads}\times
4B\approx6G\times num_{heads}\)</span></p>
<p>对于 800x800
分辨率的图片，8头注意力机制计算的显存开销是48G，这个开销显然是很难接受的。</p>
<p>从上面的分析可以看出来，输入图片的分辨率越高，Attention
计算的显存占用就越高，所以用 ViT 处理高分辨率的图片是比较困难的。</p>
<hr />
<h2 id="二pvt-的网络结构"><strong>二、PVT 的网络结构</strong></h2>
<h3 id="引入金字塔结构"><strong>引入金字塔结构</strong></h3>
<p>计算机视觉中CNN backbone经过多年的发展，沉淀了一些通用的设计模式。
最为典型的就是金字塔结构。 简单的概括就是：</p>
<p>1）feature map的分辨率随着网络加深，逐渐减小；</p>
<p>2）feature map的channel数随着网络加深，逐渐增大。</p>
<figure>
<img
src="https://fsq.oss-cn-hangzhou.aliyuncs.com/image-20220509154148002.png" srcset="/img/loading.gif" lazyload
alt="网络结构对比" />
<figcaption aria-hidden="true">网络结构对比</figcaption>
</figure>
<p>上图从左到右，我们可以看到</p>
<ul>
<li>CNN：金字塔结构，网络越深，feature map 尺寸越小，channel
数越多。</li>
<li>ViT：柱状结构，单一的 feature map 尺寸。</li>
<li>PVT：金字塔结构，网络越深，feature map 尺寸越小，channel
数越多。</li>
</ul>
<h3 id="pvt-的总体结构"><strong>PVT 的总体结构</strong></h3>
<blockquote>
<p>金字塔结构怎么才能引入到Transformer里面呢？</p>
<p>作者最终还是发现：简单地堆叠多个独立的Transformer
encoder效果是最好的。</p>
</blockquote>
<figure>
<img
src="https://fsq.oss-cn-hangzhou.aliyuncs.com/image-20220509154223571.png" srcset="/img/loading.gif" lazyload
alt="PVT" />
<figcaption aria-hidden="true">PVT</figcaption>
</figure>
<p>观察上图，不同于 ViT 在输入端做了 patch_size=16 的切图，PVT
在输入端用了 patch_size=4 分割图片，后面每个 stage 的 patch_size
都是2，分了多个 stage 进行特征图的下采样。</p>
<p>每个 block 的内部结构主要分为两个部分：切图、Transformer
Encoder。其中切图部分和 ViT 输入端类似，这里主要的创新是 Transformer
Encoder 里的 Spatial Reduction Attention (SRA)。</p>
<h3 id="sra">SRA</h3>
<p>做 SRA 的目的是减少 Attention 的显存占用。</p>
<figure>
<img
src="https://fsq.oss-cn-hangzhou.aliyuncs.com/image-20220509154426277.png" srcset="/img/loading.gif" lazyload
alt="MHA vs SRA" />
<figcaption aria-hidden="true">MHA vs SRA</figcaption>
</figure>
<p>对于 Multi-head Attention (MHA)， QKV 的维度是 <span
class="math inline">\((H_{i}W_{i}\times C_{i})\)</span> attention
计算公式为$ Attention(Q,K,V) = Softmax()V$</p>
<p>对于 SRA，Q 的维度不变还是<span
class="math inline">\((H_{i}W_{i}\times C_{i})\)</span>，KV的维度是<span
class="math inline">\(\frac{H_{i}W_{i}}{R^2_i}\times C_{i}\)</span>其中
R 是特征图切图的比例。</p>
<ul>
<li>SRA 的计算包含了多个 Attention head：<span
class="math inline">\(SRA(Q,K,V)=Concat(head_0,…,head_{N_i})W^o\)</span></li>
<li>上式每个 Attention head 的计算为：<span
class="math inline">\(head=Attention(QW^{Q}_{j},SR(K)W^{K}_{j},SR(V)W^{V}_{j})\)</span>这里对
KV 进行了下采样</li>
<li>上式 Attention 的计算公式不变，还是：$ Attention(Q,K,V) =
Softmax()V$</li>
</ul>
<blockquote>
<p>总结：为了在保证feature
map分辨率和全局感受野的同时降低计算量，把key（K）和value（V）的长和宽分别缩小到以前的<span
class="math inline">\(1/R_i\)</span>。通过这种方法，就可以以一个较小的代价处理4-stride，和8-stride的feature
map了。</p>
</blockquote>
<p>如何对 KV 进行下采样呢？这里做的也是切图，如下图所示，对 KV
大小为<span
class="math inline">\(H_iW_i\)</span>的原始特征图进行切分，每份都经过一个全连接层，最终输出的特征图大小为<span
class="math inline">\(\frac{H_iW_i}{R^2_i}\)</span></p>
<figure>
<img
src="https://fsq.oss-cn-hangzhou.aliyuncs.com/image-20220509155018674.png" srcset="/img/loading.gif" lazyload
alt="Original Patch Embedding" />
<figcaption aria-hidden="true">Original Patch Embedding</figcaption>
</figure>
<h2 id="实验对比">实验对比</h2>
<ul>
<li><strong>分类任务上对比 ViT 没有明显优势</strong></li>
<li><strong>作为骨干网络在目标检测和实例分割上优于 ResNet
系列</strong></li>
</ul>
<h2 id="观点">观点</h2>
<p><strong>1. 为什么PVT在同样参数量下比CNN效果好？</strong>
我认为有两点1）全局感受野和2）动态权重。</p>
<p>其实本质上，Multi-Head
Attention（MHA）和Conv有一些相通的地方。MHA可以大致看作是一个具备全局感受野的，且结果是按照attention
weight加权平均的卷积。因此Transformer的特征表达能力会更强。</p>
<p>关于MHA和Conv之间的联系，更多有意思的见解可以拜读下代季峰大佬的<em>An
Empirical Study of Spatial Attention Mechanisms in Deep Networks</em>。
<strong>2. 后续可扩展的思路</strong></p>
<p>1）效率更高的Attention：随着输入图片的增大，PVT消耗资源的增长率要比ResNet要高，所以PVT更适合处理中等输入分辨率的图片（具体见PVT的Ablation
Study）。所以找到一种效率更高的Attention方案是很重要的。</p>
<p>2）Position Embedding：PVT的position
embedding是和ViT一样，都是随机的参数，然后硬学的。而且在改变输入图像的分辨率的时候，position
embedding还需要通过插值来调整大小。所以我觉得这也是可以改进的地方，找到一种更适合2D图像的方法。</p>
<p>3）金字塔结构：PVT只是一种较简单的金字塔式Tranformer。中间是通过Patch
Embedding连接的，或许有更优美的方案。</p>
<hr />
<h1 id="pvtv2"><strong>PVTv2</strong></h1>
<blockquote>
<p>研究人员在PVTv1的基础上主要进行了三个方面的改进，包括构建重叠片元嵌入表达、添加卷积前传网络模块以及线性复杂度的注意力层来提升模型性能</p>
</blockquote>
<h2
id="改进的金字塔视觉transformer-pvtv2"><strong>改进的金字塔视觉Transformer
(PVTv2)</strong></h2>
<p>PVTv1是首个基于金字塔结构的视觉 Transformer
架构，提出了包含4个层级的Transformer，使用纯粹的 Transformer
主干网络在多种视觉任务上都取得了优异的性能指标。但作为第一代视觉
Transformer 架构，处于探索先驱阶段的 PVTv1 与ViT
一样<strong>存在着一些局限性。</strong></p>
<blockquote>
<p>PVTv1系列的模型架构，其中图像片元彼此不重叠造成了局域信息不连续性。</p>
</blockquote>
<h3 id="局限">局限</h3>
<ol type="1">
<li>首先，与 ViT 相同 PVTv1
将图像视为一系列非重叠的片元序列，这样对图像的<strong>非重叠切片</strong>与编码会在一定程度上<strong>损失图像中原有的连续性</strong></li>
<li>其次，PVTv1
中的位置编码为<strong>固定尺寸</strong>，对于任意尺度的图像处理缺乏灵活性。</li>
<li>当高分辨率的图像输入时，PVTv1
的计算复杂度就会飙升，大大降低了模型的性能。</li>
</ol>
<h3
id="从以下三个方面对原有模型进行改进">从以下三个方面对原有模型进行改进</h3>
<figure>
<img
src="https://fsq.oss-cn-hangzhou.aliyuncs.com/image-20220510103323935.png" srcset="/img/loading.gif" lazyload
alt="Two improvements in PVTv2" />
<figcaption aria-hidden="true">Two improvements in PVTv2</figcaption>
</figure>
<ol type="1">
<li><p>重叠片元 (patch) 嵌入编码。在 PVTv2
中使用了重叠片元嵌入来对图像进行编码。上图左侧展示了重叠片元操作的示意图，图中<strong>每个片元的窗口尺寸被放大、与相邻窗口互相重叠一半</strong>，同时对特征图进行0填充操作。而后使用卷积对填充后特征图进操作实现嵌入编码。具体来讲，给定
$ hwc$
的输入，应用步长为S、尺寸为2S-1的卷积，填充大小为S-1，使用c’个卷积核最终获得$
c$ 的输出结果。</p></li>
<li><p>卷积前传。为了解决图像大小灵活性的问题，在新版的 PVT
中固定位置编码被移除，并引入了填充零的位置编码机制，上图右侧显示了在前传网络和全连接层间插入的3x3零填充逐深度卷积。</p></li>
<li><p>线性空间缩减注意力机制。<strong>为了进一步优化PVT的计算开销，线性空间注意力
(Spatial Reduction Attention, SRA)
机制被引入到新的模型中来。</strong>与原始的SRA不同，线性空间注意力机制具有线性的计算复杂度和内存开销，对于输入为hxwxc的特征图，与SRA相比线性SRA的复杂度大大降低：</p>
<figure>
<img
src="https://fsq.oss-cn-hangzhou.aliyuncs.com/image-20220510103400765.png" srcset="/img/loading.gif" lazyload
alt="Comparison of SRA in PVTv1 and linear SRA in PVTv2" />
<figcaption aria-hidden="true">Comparison of SRA in PVTv1 and linear SRA
in PVTv2</figcaption>
</figure>
<figure>
<img
src="https://fsq.oss-cn-hangzhou.aliyuncs.com/image-20220510103503906.png" srcset="/img/loading.gif" lazyload
alt="复杂度计算公式" />
<figcaption aria-hidden="true">复杂度计算公式</figcaption>
</figure>
<p>其中，其中R是SRA的空间缩减比例，P是线性SRA中的池化大小，在PVTv2中被设置为7。</p></li>
</ol>
<p>通过这三方面的改进，PVTv2不仅可以保证图像和特征图的局域连续性，同时可灵活处理不同尺度的输入信号，还能将计算复杂度控制在线性范围内。</p>
<p>为了充分探索 PVTv2 的实验性能，下表列出了一系列配置下的 PVTv2
模型家族，其中S、C、R、P、N、E、L分别代表了重叠片元的步长、输出通道数、SRA的缩减比例、池化大小、注意力头数量、前传网络层的拓展比例、编码器的层数。</p>
<figure>
<img
src="https://fsq.oss-cn-hangzhou.aliyuncs.com/image-20220510103537230.png" srcset="/img/loading.gif" lazyload
alt="Detailed settings of PVTv2 series. “-Li” denotes PVTv2 with linear SRA" />
<figcaption aria-hidden="true">Detailed settings of PVTv2 series. “-Li”
denotes PVTv2 with linear SRA</figcaption>
</figure>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E8%AE%BA%E6%96%87/">论文</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Transformer/">Transformer</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/05/06/PyramidTNT/">
                        <span class="hidden-mobile">PyramidTNT</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"ezAjSCt600hHMv0BReBYtgEg-gzGzoHsz","appKey":"ED6lqUmM45dvAhwFci9k1ioL","path":"window.location.pathname","placeholder":"欢迎留言呀~","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"enable":true},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          Fluid.plugins.initFancyBox('#valine .vcontent img:not(.vemoji)');
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> # <i class="iconfont icon-love"></i> <i> | </i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <div>方小七的博客 | 记录学习的过程</div>

  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  








  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
