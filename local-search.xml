<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>MAE</title>
    <link href="/2022/04/11/MAE/"/>
    <url>/2022/04/11/MAE/</url>
    
    <content type="html"><![CDATA[<h1 id="MAE"><a href="#MAE" class="headerlink" title="MAE"></a>MAE</h1><h2 id="与之前的Transformer有什么区别？"><a href="#与之前的Transformer有什么区别？" class="headerlink" title="与之前的Transformer有什么区别？"></a>与之前的Transformer有什么区别？</h2><p>Transformer</p><p>Bert：NLP领域，相当于把一些词挖掉，在训练模型做完形填空</p><p>ViT</p><p>MAE：可以看作是一个将Bert拓展到CV</p><h2 id="一、标题"><a href="#一、标题" class="headerlink" title="一、标题"></a>一、标题</h2><ul><li>auto指的是“自”的意思，标号是图片自己本身</li><li>scalable：规模大（efficient来形容速度快）</li></ul><h2 id="二、摘要"><a href="#二、摘要" class="headerlink" title="二、摘要"></a>二、摘要</h2><p>“we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task.” (He 等。, 2021, p. 1)</p><ul><li>设计一个非对称的encoder-decoder</li><li>编码器只作用于可见的patch中</li><li>如果遮住了较多部分如75%，那么他会得到一个非显然的而且有意义的自监督的任务</li></ul><p>“Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.” (He 等。, 2021, p. 1)</p><p>该研究主要是用来做迁移学习的。</p><h2 id="三、关键图"><a href="#三、关键图" class="headerlink" title="三、关键图"></a>三、关键图</h2><p><img src="https://fsq.oss-cn-hangzhou.aliyuncs.com/image-20220411184342734.png" alt="image-20220411184342734"></p><ul><li>灰色，表示盖住</li><li>没有被盖住的拿出来，放入encoder（ViT）：主要计算量来自于编码器</li><li>得到每一个块对应的特征</li><li>把被盖住的块，重新放回原来的位置（原始图拉成了一个向量）</li><li>输入到解码器decoder</li><li>解码器重构原来的信息</li></ul><p><img src="https://fsq.oss-cn-hangzhou.aliyuncs.com/image-20220411181112127.png" alt="image-20220411181112127"></p><p>第一列是打了mask  第二列是重构  第三列是原图</p><p><img src="https://fsq.oss-cn-hangzhou.aliyuncs.com/image-20220411181141648.png" alt="image-20220411181141648"></p><p>mask不同比例的演示效果</p><h2 id="四、引言"><a href="#四、引言" class="headerlink" title="四、引言"></a>四、引言</h2><p><em>研究问题</em></p><ul><li>是什么让自编码mask在视觉和语言领域效果不同？</li></ul><p><em>对应提出观点</em></p><ol><li>CV通常使用卷积扫来扫去，卷积窗口很难区分边界，最后难还原</li><li>信息的密度有点不一样，图片信息冗余，可能可以通过插值插回来</li><li>在视觉中，解码器仅仅使用一个MLP是不够的</li></ol><p><strong>写作手法</strong></p><p>提问题-回答问题-引导出创新</p><p>以这样的结构是来解释为什么要设计成这个样子的原因讲清楚了</p><h2 id="五、相关工作"><a href="#五、相关工作" class="headerlink" title="五、相关工作"></a>五、相关工作</h2><ul><li>“Masked language modeling”</li><li>“Autoencoding”</li><li>“Masked image encoding”</li><li>“Self-supervised learning”</li></ul><h2 id="六、方法"><a href="#六、方法" class="headerlink" title="六、方法"></a>六、方法</h2><p>干货章节</p><ol><li><p>介绍掩码是怎么工作的</p><p> 只采样少量的块</p></li><li><p>MAE encoder 和ViT 差不多</p><p> 被mask的就不传入了</p></li><li><p>MAE decoder（主要作用做在预训练的时候）</p><p> 其实就是另外一个transformer</p></li><li><p>怎么样重构出原来的像素</p></li><li><p>简单实现</p><ul><li>首先生成token列，做一次线性投影，再加上位置信息</li><li>把这个序列随机打乱一下，把最后一块拿掉（相当于打乱之后保留前面25%，后面的丢掉）【随机采样】</li><li>在解码的时候要在后面附上跟之前长度一样的mask的patches</li><li>重新unshuffle一下</li></ul></li></ol><h2 id="七、实验"><a href="#七、实验" class="headerlink" title="七、实验"></a>七、实验</h2><ol><li><p>ImageNet上的实验</p><p> “with strong regularization” 在之前ViT的论文中说需要一个比较大的数据集才能够得到好的效果，后来研究发现如果加入合适的正则项的话，其实在小一点的数据上也是能训练出来的。</p><ul><li>无监督的训练</li><li>有监督的训练</li></ul></li><li><p>各种消融实验（不同超参数）</p></li><li><p>比的是与其他方法比</p></li><li><p>局部微调【高层于自己的任务相关，最好调一调】</p></li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol><li><p>简单算法？（主要是针对该领域的研究）</p><p> MAE基于ViT,ViT基于Transformer模型算法不会简单</p></li><li><p>效果可以媲美有标号的工作</p></li><li><p>潜在影响【类似于说GAN】,可能会有不存在的东西出现</p></li></ol><h2 id="个人思考："><a href="#个人思考：" class="headerlink" title="个人思考："></a>个人思考：</h2><p>1、嫁接这个创新点到我们自己的舞台数据集上</p><p>将图片打成patches,因为有关键点标签。可以将关键点所在的patches保留，其他图片块mask掉。</p><p>2、代码没有公布，用的是TensorFlow+TPU，小老百姓难嫁接</p><p>3、随机mask将重要信息mask了，是否应该选择性的mask?</p>]]></content>
    
    
    <categories>
      
      <category>论文</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
